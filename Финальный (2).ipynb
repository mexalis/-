{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m5nG3H3xancr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def read_binarry(file_path):\n",
        "    import struct\n",
        "    import math\n",
        "\n",
        "    power = []\n",
        "    age = []\n",
        "    coordinate_x = []\n",
        "    coordinate_y = []\n",
        "    angle_tetta = []\n",
        "    angle_phi = []\n",
        "    energy = []\n",
        "    time = []\n",
        "\n",
        "    with open(file_path, 'rb') as binary_file:\n",
        "        for i in range(100000):\n",
        "            binary_file.read(4 * 5)\n",
        "\n",
        "            tetta = struct.unpack('f', binary_file.read(4))[0]\n",
        "            angle_tetta.append(tetta)\n",
        "\n",
        "            phi = struct.unpack('f', binary_file.read(4))[0]\n",
        "            angle_phi.append(phi)\n",
        "\n",
        "            x0 = struct.unpack('f', binary_file.read(4))[0]\n",
        "            coordinate_x.append(x0)\n",
        "\n",
        "            y0 = struct.unpack('f', binary_file.read(4))[0]\n",
        "            coordinate_y.append(y0)\n",
        "\n",
        "            binary_file.read(4 * 5)\n",
        "\n",
        "            power_eas = struct.unpack('f', binary_file.read(4))[0]\n",
        "            power.append(math.log10(power_eas))\n",
        "\n",
        "            age_eas = struct.unpack('f', binary_file.read(4))[0]\n",
        "            age.append(age_eas)\n",
        "\n",
        "            binary_file.read(4 * 1565)\n",
        "            energy_release = struct.unpack('f' * 36, binary_file.read(4 * 36))\n",
        "            energy.append(energy_release)\n",
        "\n",
        "            binary_file.read(4)\n",
        "            t = struct.unpack('f' * 144, binary_file.read(4 * 144))\n",
        "            threshold_time = t[::4]\n",
        "            time.append(threshold_time)\n",
        "\n",
        "    # Собираем всё в DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'power': power,\n",
        "        'age': age,\n",
        "        'x': coordinate_x,\n",
        "        'y': coordinate_y,\n",
        "        'tetta': angle_tetta,\n",
        "        'phi': angle_phi,\n",
        "        'energy': energy,\n",
        "        'threshold_time': time,\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Заморозка 42 (52)\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "set_seed(42)\n",
        "random_state = 42"
      ],
      "metadata": {
        "id": "Y0XvS8jgayJ7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = read_binarry(\"/content/drive/MyDrive/Nuclear IT Hack/spe27p_100k_2022_correct.dat\")\n",
        "\n",
        "new_columns = pd.DataFrame(\n",
        "    df['energy'].tolist(),\n",
        "    columns=[f'energy_{i}' for i in range(36)]\n",
        ")\n",
        "\n",
        "df = pd.concat([df, new_columns], axis=1)\n",
        "\n",
        "new_columns = pd.DataFrame(\n",
        "    df['threshold_time'].tolist(),\n",
        "    columns=[f'threshold_time_{i}' for i in range(36)]\n",
        ")\n",
        "\n",
        "df = pd.concat([df, new_columns], axis=1)\n",
        "\n",
        "df.drop(columns=[\"energy\", \"threshold_time\"], inplace=True)\n",
        "\n",
        "X = df.drop(columns=[\"power\", \"age\", \"x\", \"y\", \"tetta\", \"phi\"])"
      ],
      "metadata": {
        "id": "j53ThwGSa4Vi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bctrFFvqa7p-",
        "outputId": "5e10f35b-51ed-47f6-81da-419890f3b8f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "r78c7r9KbIsZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_scaler = StandardScaler()\n",
        "X = x_scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "ICznIhxwbgE4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_power = df[\"power\"]"
      ],
      "metadata": {
        "id": "cjN7-QtmbraH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_power_train, y_power_test = train_test_split(X, y_power, test_size=0.3, random_state=random_state)\n",
        "train_data = lgb.Dataset(X_train, label=y_power_train)\n",
        "test_data = lgb.Dataset(X_test, label=y_power_test, reference=train_data)"
      ],
      "metadata": {
        "id": "JK66zsBRb0la"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'mse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 10,\n",
        "    'learning_rate': 0.07,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 1,\n",
        "    'n_jobs': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "model_lgb_power = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    num_boost_round=100,\n",
        "    valid_sets=[train_data, test_data],\n",
        "    valid_names=['train', 'valid']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HYu8Sq6b5fJ",
        "outputId": "32b49f7c-aa26-4c0f-be8e-e733ba5aed97"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061915 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 18360\n",
            "[LightGBM] [Info] Number of data points in the train set: 70000, number of used features: 72\n",
            "[LightGBM] [Info] Start training from score 4.642989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hqBn1w7XggnA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_power_preds = model_lgb_power.predict(X_test, num_iteration=model_lgb_power.best_iteration)\n",
        "evaluate_regression(y_power_test, y_power_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnNJlBKvcBoa",
        "outputId": "6b8da44d-d9b8-4c75-a2ac-ffb8616258f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.1202\n",
            "MAE: 0.0932\n",
            "R²: 0.9619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = df[\"power\"]"
      ],
      "metadata": {
        "id": "oz9UDXBChe7H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_em = []\n",
        "for i in range(100000):\n",
        "  ser = X[i]\n",
        "  row = [[ser[i] for i in range(36)], [ser[i] for i in range(36, 72)]]\n",
        "  # for i in range(36):\n",
        "  #   row.append([ser[i], ser[36+i]])\n",
        "  X_em.append(row)\n",
        "X_em = np.array(X_em)\n",
        "X_em = X_em.astype(np.float32)"
      ],
      "metadata": {
        "id": "v5J5AOEqdHlM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "39mQI4sigriD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_em, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "G8gg5riygvmQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(\n",
        "    torch.tensor(X_train.astype(np.float32)),\n",
        "    torch.tensor(y_train.values.astype(np.float32))\n",
        ")\n",
        "val_dataset = TensorDataset(\n",
        "    torch.tensor(X_val.astype(np.float32)),\n",
        "    torch.tensor(y_val.values.astype(np.float32))\n",
        ")\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "class TransformerRegressor(nn.Module):\n",
        "    def __init__(self, num_features=2, d_model=36, nhead=9, num_layers=6, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.pos_encoder = nn.Parameter(torch.randn(1, num_features, d_model))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dropout=dropout, dim_feedforward=4*d_model\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(d_model, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = x.unsqueeze(-1)  # [batch, num_features, 1]\n",
        "        #x = self.embedding(x)  # [batch, num_features, d_model]\n",
        "        x += self.pos_encoder\n",
        "        x = x.permute(1, 0, 2)  # [num_features, batch, d_model]\n",
        "        x = self.encoder(x)  # [num_features, batch, d_model]\n",
        "        x = x.permute(1, 2, 0)  # [batch, d_model, num_features]\n",
        "        x = self.pool(x).squeeze(-1)  # [batch, d_model]\n",
        "        return self.regressor(x).squeeze(-1)  # [batch]"
      ],
      "metadata": {
        "id": "VK2DAU2_gxWL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerRegressor()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N76UMW26g2fA",
        "outputId": "f1e333da-650e-4869-a2ae-d288360144e7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "-BdrN5L3g3XN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjL1DoPvg5hW",
        "outputId": "00feabdf-95d6-4d83-cbf0-94e15cb6263f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 9\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Train MSE: {train_loss:.6f}, Val MSE: {val_loss:.6f}\")\n",
        "    print(f\"  Train RMSE: {np.sqrt(train_loss):.6f}, Val RMSE: {np.sqrt(val_loss):.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vBoW7tGg623",
        "outputId": "c897c841-8b11-437b-82ad-db8a96f7d902"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9:\n",
            "  Train MSE: 1.033871, Val MSE: 0.061246\n",
            "  Train RMSE: 1.016794, Val RMSE: 0.247480\n",
            "Epoch 2/9:\n",
            "  Train MSE: 0.045042, Val MSE: 0.029442\n",
            "  Train RMSE: 0.212230, Val RMSE: 0.171587\n",
            "Epoch 3/9:\n",
            "  Train MSE: 0.033383, Val MSE: 0.039447\n",
            "  Train RMSE: 0.182709, Val RMSE: 0.198614\n",
            "Epoch 4/9:\n",
            "  Train MSE: 0.030227, Val MSE: 0.042351\n",
            "  Train RMSE: 0.173858, Val RMSE: 0.205793\n",
            "Epoch 5/9:\n",
            "  Train MSE: 0.027945, Val MSE: 0.023719\n",
            "  Train RMSE: 0.167167, Val RMSE: 0.154010\n",
            "Epoch 6/9:\n",
            "  Train MSE: 0.026192, Val MSE: 0.022282\n",
            "  Train RMSE: 0.161838, Val RMSE: 0.149273\n",
            "Epoch 7/9:\n",
            "  Train MSE: 0.024745, Val MSE: 0.020429\n",
            "  Train RMSE: 0.157307, Val RMSE: 0.142929\n",
            "Epoch 8/9:\n",
            "  Train MSE: 0.023378, Val MSE: 0.020233\n",
            "  Train RMSE: 0.152899, Val RMSE: 0.142242\n",
            "Epoch 9/9:\n",
            "  Train MSE: 0.023053, Val MSE: 0.021555\n",
            "  Train RMSE: 0.151832, Val RMSE: 0.146817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_regression(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "\n",
        "def predict_data(model, data, device):\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data = data.values\n",
        "\n",
        "    X_new_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X_new_tensor)\n",
        "        return predictions.cpu().numpy()\n",
        "\n",
        "\n",
        "pred = predict_data(model, X_val, device)\n",
        "evaluate_regression(pred, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz1V9sO-hF_C",
        "outputId": "c48c9f16-f725-4c82-fff4-9cd77383a94c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.0216\n",
            "R²: 0.9409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "^ наверху статистика по power"
      ],
      "metadata": {
        "id": "mu94BgVfiCig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df[\"x\"]\n",
        "\n",
        "X_em = []\n",
        "for i in range(100000):\n",
        "  ser = X[i]\n",
        "  row = [[ser[i] for i in range(36)], [ser[i] for i in range(36, 72)]]\n",
        "  # for i in range(36):\n",
        "  #   row.append([ser[i], ser[36+i]])\n",
        "  X_em.append(row)\n",
        "X_em = np.array(X_em)\n",
        "X_em = X_em.astype(np.float32)\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_em, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(\n",
        "    torch.tensor(X_train.astype(np.float32)),\n",
        "    torch.tensor(y_train.values.astype(np.float32))\n",
        ")\n",
        "val_dataset = TensorDataset(\n",
        "    torch.tensor(X_val.astype(np.float32)),\n",
        "    torch.tensor(y_val.values.astype(np.float32))\n",
        ")\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "model = TransformerRegressor()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "num_epochs = 9\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Train MSE: {train_loss:.6f}, Val MSE: {val_loss:.6f}\")\n",
        "    print(f\"  Train RMSE: {np.sqrt(train_loss):.6f}, Val RMSE: {np.sqrt(val_loss):.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3qVeXsChYuE",
        "outputId": "308b4df5-6fff-4553-d1fa-ccd3d388b5eb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9:\n",
            "  Train MSE: 252.652076, Val MSE: 145.087123\n",
            "  Train RMSE: 15.895033, Val RMSE: 12.045212\n",
            "Epoch 2/9:\n",
            "  Train MSE: 149.638761, Val MSE: 128.982910\n",
            "  Train RMSE: 12.232692, Val RMSE: 11.357064\n",
            "Epoch 3/9:\n",
            "  Train MSE: 129.084083, Val MSE: 130.753971\n",
            "  Train RMSE: 11.361518, Val RMSE: 11.434770\n",
            "Epoch 4/9:\n",
            "  Train MSE: 120.208522, Val MSE: 115.709928\n",
            "  Train RMSE: 10.963965, Val RMSE: 10.756855\n",
            "Epoch 5/9:\n",
            "  Train MSE: 112.444774, Val MSE: 108.889759\n",
            "  Train RMSE: 10.603998, Val RMSE: 10.435026\n",
            "Epoch 6/9:\n",
            "  Train MSE: 111.688627, Val MSE: 121.822665\n",
            "  Train RMSE: 10.568284, Val RMSE: 11.037331\n",
            "Epoch 7/9:\n",
            "  Train MSE: 107.704162, Val MSE: 112.231896\n",
            "  Train RMSE: 10.378062, Val RMSE: 10.593956\n",
            "Epoch 8/9:\n",
            "  Train MSE: 105.194487, Val MSE: 99.226722\n",
            "  Train RMSE: 10.256436, Val RMSE: 9.961261\n",
            "Epoch 9/9:\n",
            "  Train MSE: 103.412797, Val MSE: 116.155113\n",
            "  Train RMSE: 10.169208, Val RMSE: 10.777528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_regression(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "\n",
        "def predict_data(model, data, device):\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data = data.values\n",
        "\n",
        "    X_new_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X_new_tensor)\n",
        "        return predictions.cpu().numpy()\n",
        "\n",
        "\n",
        "pred = predict_data(model, X_val, device)\n",
        "evaluate_regression(pred, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAzgU27PvfSj",
        "outputId": "572f0da6-a200-4a6f-8ba9-0e8ae7f418c8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 116.1551\n",
            "R²: 0.7341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "^ выше статистика по x"
      ],
      "metadata": {
        "id": "nlsYMGzEvr9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df[\"age\"]\n",
        "\n",
        "X_em = []\n",
        "for i in range(100000):\n",
        "  ser = X[i]\n",
        "  row = [[ser[i] for i in range(36)], [ser[i] for i in range(36, 72)]]\n",
        "  # for i in range(36):\n",
        "  #   row.append([ser[i], ser[36+i]])\n",
        "  X_em.append(row)\n",
        "X_em = np.array(X_em)\n",
        "X_em = X_em.astype(np.float32)\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_em, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(\n",
        "    torch.tensor(X_train.astype(np.float32)),\n",
        "    torch.tensor(y_train.values.astype(np.float32))\n",
        ")\n",
        "val_dataset = TensorDataset(\n",
        "    torch.tensor(X_val.astype(np.float32)),\n",
        "    torch.tensor(y_val.values.astype(np.float32))\n",
        ")\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "model = TransformerRegressor()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "num_epochs = 9\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Train MSE: {train_loss:.6f}, Val MSE: {val_loss:.6f}\")\n",
        "    print(f\"  Train RMSE: {np.sqrt(train_loss):.6f}, Val RMSE: {np.sqrt(val_loss):.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f0ixa5WiO4p",
        "outputId": "25f8bf78-2988-4f1d-916b-3715ca9a64f7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9:\n",
            "  Train MSE: 0.023531, Val MSE: 0.004754\n",
            "  Train RMSE: 0.153397, Val RMSE: 0.068947\n",
            "Epoch 2/9:\n",
            "  Train MSE: 0.004190, Val MSE: 0.004387\n",
            "  Train RMSE: 0.064730, Val RMSE: 0.066231\n",
            "Epoch 3/9:\n",
            "  Train MSE: 0.003901, Val MSE: 0.003732\n",
            "  Train RMSE: 0.062462, Val RMSE: 0.061090\n",
            "Epoch 4/9:\n",
            "  Train MSE: 0.003371, Val MSE: 0.004032\n",
            "  Train RMSE: 0.058058, Val RMSE: 0.063496\n",
            "Epoch 5/9:\n",
            "  Train MSE: 0.003223, Val MSE: 0.003246\n",
            "  Train RMSE: 0.056770, Val RMSE: 0.056972\n",
            "Epoch 6/9:\n",
            "  Train MSE: 0.003158, Val MSE: 0.003667\n",
            "  Train RMSE: 0.056198, Val RMSE: 0.060559\n",
            "Epoch 7/9:\n",
            "  Train MSE: 0.003113, Val MSE: 0.003298\n",
            "  Train RMSE: 0.055798, Val RMSE: 0.057430\n",
            "Epoch 8/9:\n",
            "  Train MSE: 0.003096, Val MSE: 0.003452\n",
            "  Train RMSE: 0.055641, Val RMSE: 0.058751\n",
            "Epoch 9/9:\n",
            "  Train MSE: 0.003081, Val MSE: 0.003190\n",
            "  Train RMSE: 0.055504, Val RMSE: 0.056477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_regression(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "\n",
        "def predict_data(model, data, device):\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        data = data.values\n",
        "\n",
        "    X_new_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X_new_tensor)\n",
        "        return predictions.cpu().numpy()\n",
        "\n",
        "\n",
        "pred = predict_data(model, X_val, device)\n",
        "evaluate_regression(pred, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiCDtf_3iiAC",
        "outputId": "39bf00b5-9eab-4670-9ff7-d066e3fff09a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.0032\n",
            "R²: -1.5698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ^это не учитывать, просто эксперимент"
      ],
      "metadata": {
        "id": "5UU2XC0GvbQV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# y = df[\"age\"]\n",
        "\n",
        "# # X_em = []\n",
        "# # for i in range(100000):\n",
        "# #   ser = X.loc[i]\n",
        "# #   row = [[ser[i] for i in range(36)], [ser[i] for i in range(36, 72)]]\n",
        "# #   # for i in range(36):\n",
        "# #   #   row.append([ser[i], ser[36+i]])\n",
        "# #   X_em.append(row)\n",
        "# X_em = np.array(X_em)\n",
        "# X_em = X_em.astype(np.float32)\n",
        "\n",
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(\n",
        "#     X_em, y, test_size=0.2, random_state=42\n",
        "# )\n",
        "\n",
        "\n",
        "# train_dataset = TensorDataset(\n",
        "#     torch.tensor(X_train.astype(np.float32)),\n",
        "#     torch.tensor(y_train.values.astype(np.float32))\n",
        "# )\n",
        "# val_dataset = TensorDataset(\n",
        "#     torch.tensor(X_val.astype(np.float32)),\n",
        "#     torch.tensor(y_val.values.astype(np.float32))\n",
        "# )\n",
        "# batch_size = 256\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# model = TransformerRegressor()\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = model.to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "# criterion = nn.MSELoss()\n",
        "\n",
        "# num_epochs = 9\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     train_loss = 0\n",
        "#     for X_batch, y_batch in train_loader:\n",
        "#         X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(X_batch)\n",
        "#         loss = criterion(outputs, y_batch)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         print(outputs)\n",
        "#         train_loss += loss.item() * X_batch.size(0)\n",
        "#         #print(X_batch.size(0), loss.item())\n",
        "#     model.eval()\n",
        "#     val_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for X_batch, y_batch in val_loader:\n",
        "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "#             outputs = model(X_batch)\n",
        "#             loss = criterion(outputs, y_batch)\n",
        "#             val_loss += loss.item() * X_batch.size(0)\n",
        "#     print(len(train_loader.dataset), train_loss)\n",
        "#     train_loss /= len(train_loader.dataset)\n",
        "#     val_loss /= len(val_loader.dataset)\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "#     print(f\"  Train MSE: {train_loss:.6f}, Val MSE: {val_loss:.6f}\")\n",
        "#     print(f\"  Train RMSE: {np.sqrt(train_loss):.6f}, Val RMSE: {np.sqrt(val_loss):.6f}\")\n",
        "\n",
        "# def evaluate_regression(y_true, y_pred):\n",
        "#     mse = mean_squared_error(y_true, y_pred)\n",
        "#     r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "#     print(f\"MSE: {mse:.4f}\")\n",
        "#     print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "\n",
        "# def predict_data(model, data, device):\n",
        "#     model.eval()\n",
        "\n",
        "#     if isinstance(data, pd.DataFrame):\n",
        "#         data = data.values\n",
        "\n",
        "#     X_new_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         predictions = model(X_new_tensor)\n",
        "#         return predictions.cpu().numpy()\n",
        "\n",
        "\n",
        "# pred = predict_data(model, X_val, device)\n",
        "# evaluate_regression(pred, y_val)"
      ],
      "metadata": {
        "id": "lWiUuV8mkfSB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "X = df.drop(columns=[\"power\", \"age\", \"x\", \"y\", \"tetta\", \"phi\"])\n",
        "y = df['age']\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "params = {\n",
        "    'objective': 'regression',       # Тип задачи - регрессия\n",
        "    'metric': 'rmse',                # Метрика качества (Root Mean Squared Error)\n",
        "    'boosting_type': 'gbdt',         # Алгоритм градиентного бустинга\n",
        "    'num_leaves': 31,                # Максимальное количество листьев в дереве\n",
        "    'learning_rate': 0.05,           # Скорость обучения\n",
        "    'feature_fraction': 0.9,         # Доля случайно выбираемых признаков на каждой итерации\n",
        "    'bagging_fraction': 0.8,         # Доля данных для бутстрепа\n",
        "    'bagging_freq': 6,               # Частота бэггинга\n",
        "    'verbose': 1,                    # Отключение выводов\n",
        "    'n_jobs': 1,                    # Использовать все ядра процессора\n",
        "    'random_state': 42               # Для воспроизводимости\n",
        "}\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    num_boost_round=100,            # Максимальное количество деревьев\n",
        "    valid_sets=[train_data, test_data],\n",
        "    valid_names=['train', 'valid']\n",
        ")\n",
        "y_preds = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "metrics = evaluate_regression(y_test, y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY783YzWnMHd",
        "outputId": "dfc142f0-9dd2-46c3-808a-4e8bbbb88545"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061739 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 18360\n",
            "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 72\n",
            "[LightGBM] [Info] Start training from score 1.367758\n",
            "MSE: 0.0029\n",
            "R²: 0.3065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "^ выше статистики для age"
      ],
      "metadata": {
        "id": "s3Gy4M3wkyor"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gVcwMu90qjzq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tzo88DwDlbpP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "^ выше статистики для x"
      ],
      "metadata": {
        "id": "mYXyoD2Il1-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "X = df.drop(columns=[\"power\", \"age\", \"x\", \"y\", \"tetta\", \"phi\"])\n",
        "y = df['y']\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "params = {\n",
        "    'objective': 'regression',       # Тип задачи - регрессия\n",
        "    'metric': 'rmse',                # Метрика качества (Root Mean Squared Error)\n",
        "    'boosting_type': 'gbdt',         # Алгоритм градиентного бустинга\n",
        "    'num_leaves': 31,                # Максимальное количество листьев в дереве\n",
        "    'learning_rate': 0.05,           # Скорость обучения\n",
        "    'feature_fraction': 0.9,         # Доля случайно выбираемых признаков на каждой итерации\n",
        "    'bagging_fraction': 0.8,         # Доля данных для бутстрепа\n",
        "    'bagging_freq': 6,               # Частота бэггинга\n",
        "    'verbose': 1,                    # Отключение выводов\n",
        "    'n_jobs': 1,                    # Использовать все ядра процессора\n",
        "    'random_state': 42               # Для воспроизводимости\n",
        "}\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    num_boost_round=100,            # Максимальное количество деревьев\n",
        "    valid_sets=[train_data, test_data],\n",
        "    valid_names=['train', 'valid']\n",
        ")\n",
        "y_preds = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "metrics = evaluate_regression(y_test, y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMu6UT0Plx4s",
        "outputId": "6e7041c0-8814-4105-ba40-260c1d32b3ab"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064339 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 18360\n",
            "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 72\n",
            "[LightGBM] [Info] Start training from score -2.953094\n",
            "MSE: 158.4119\n",
            "R²: 0.8966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "^ выше статистики для y"
      ],
      "metadata": {
        "id": "Qz7WTkGFmZd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "X = df.drop(columns=[\"power\", \"age\", \"x\", \"y\", \"tetta\", \"phi\"])\n",
        "y = (df['tetta']*math.pi/180).agg('cos') #pd.concat([prep1,prep2], axis=1)\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "params = {\n",
        "    'objective': 'regression',       # Тип задачи - регрессия\n",
        "    'metric': 'rmse',                # Метрика качества (Root Mean Squared Error)\n",
        "    'boosting_type': 'gbdt',         # Алгоритм градиентного бустинга\n",
        "    'num_leaves': 31,                # Максимальное количество листьев в дереве\n",
        "    'learning_rate': 0.05,           # Скорость обучения\n",
        "    'feature_fraction': 0.9,         # Доля случайно выбираемых признаков на каждой итерации\n",
        "    'bagging_fraction': 0.8,         # Доля данных для бутстрепа\n",
        "    'bagging_freq': 6,               # Частота бэггинга\n",
        "    'verbose': 1,                    # Отключение выводов\n",
        "    'n_jobs': 1,                    # Использовать все ядра процессора\n",
        "    'random_state': 42               # Для воспроизводимости\n",
        "}\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    num_boost_round=100,            # Максимальное количество деревьев\n",
        "    valid_sets=[train_data, test_data],\n",
        "    valid_names=['train', 'valid']\n",
        ")\n",
        "y_preds = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "metrics = evaluate_regression(y_test, y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mdTGqr0ltXH",
        "outputId": "67a75fd7-ddfa-40b5-ccf5-d965a423b107"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072219 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 18360\n",
            "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 72\n",
            "[LightGBM] [Info] Start training from score 0.821163\n",
            "MSE: 0.0045\n",
            "R²: 0.5818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "^ выше статистика по tetta"
      ],
      "metadata": {
        "id": "wg9Eu-ZPwFLu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WYdDf8LGvRnn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "X = df.drop(columns=[\"power\", \"age\", \"x\", \"y\", \"tetta\", \"phi\"])\n",
        "y = (df['phi']*math.pi/180).agg('cos') #pd.concat([prep1,prep2], axis=1)\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "params = {\n",
        "    'objective': 'regression',       # Тип задачи - регрессия\n",
        "    'metric': 'rmse',                # Метрика качества (Root Mean Squared Error)\n",
        "    'boosting_type': 'gbdt',         # Алгоритм градиентного бустинга\n",
        "    'num_leaves': 31,                # Максимальное количество листьев в дереве\n",
        "    'learning_rate': 0.05,           # Скорость обучения\n",
        "    'feature_fraction': 0.9,         # Доля случайно выбираемых признаков на каждой итерации\n",
        "    'bagging_fraction': 0.8,         # Доля данных для бутстрепа\n",
        "    'bagging_freq': 6,               # Частота бэггинга\n",
        "    'verbose': 1,                    # Отключение выводов\n",
        "    'n_jobs': 1,                    # Использовать все ядра процессора\n",
        "    'random_state': 42               # Для воспроизводимости\n",
        "}\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_data,\n",
        "    num_boost_round=100,            # Максимальное количество деревьев\n",
        "    valid_sets=[train_data, test_data],\n",
        "    valid_names=['train', 'valid']\n",
        ")\n",
        "y_preds = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "metrics = evaluate_regression(y_test, y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZjN84wknvg-",
        "outputId": "d8be70fe-258b-4c68-b866-dc3d05b3f2cd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063898 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 18360\n",
            "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 72\n",
            "[LightGBM] [Info] Start training from score -0.001162\n",
            "MSE: 0.3648\n",
            "R²: 0.2739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "^ выше статистика по phi"
      ],
      "metadata": {
        "id": "fLKpHd1gwLEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ^ выше статистика для phi"
      ],
      "metadata": {
        "id": "x57YeH4bu9mK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l1sDUA1uu27k"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}